%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2003 International Union of Crystallography
% Version 1.2 (11 December 2002)
%------------------------------------------------------------------------------
%
\documentclass[preprint, pdf]{iucr}              % DO NOT DELETE THIS LINE
                   \def\href#1{\relax}\let\foo\caption
\ifPDF
  \RequirePackage{hyperref}
  \PassOptionsToPackage{pdftex,bookmarksopen,bookmarksnumbered}{hyperref}
  \voffset=-0.5in
\fi
\let\caption\foo

\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
 \usepackage{amsmath}
%\usepackage{eps2pdf}

 \paperprodcode{a000000}      % Replace with production code if known
 \paperref{xx9999}            % Replace xx9999 with reference code if known
 \papertype{IU}               % Indicate type of article
 \paperlang{english}          % Can be english, french, german or russian
 \journalcode{S}             % Indicate the journal to which submitted
 \journalyr{2017}
 \journalreceived{\relax}
 \journalaccepted{\relax}
 \journalonline{\relax}

\begin{document}                  % DO NOT DELETE THIS LINE

\title{Speeding-up diffraction imaging by accelerating image decompression}
\shorttitle{GPU decompression of CBF images}

 \cauthor[a]{J.}{Kieffer}{jerome.kieffer@esrf.eu}
 
 \aff[a]{ESRF, The European Synchrotron, CS40220, 38043 \city{Grenoble}
 Cedex 9, \country{France}}
 \shortauthor{Kieffer}


\keyword{Powder diffraction}
\keyword{Diffraction imaging}
\keyword{Pilatus detector}
\keyword{pyFAI}
\keyword{FabIO}
\keyword{Silx}



\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
\end{synopsis}

\begin{abstract}

Diffraction imaging is an X-Ray imaging method which uses the cristallinity
information as signal to create images pixel wise: 
a pencil beam is raster-scanned onto a sample and the (powder) diffraction
signal is record by a large area detector.
In this type of experiment, the speed of
measurement is often limited by the transfer rate or the local storage capacity
because modern area pixel-detector provide so much data and the data
reduction can hardly be done in real time.

This contribution presents the benchmarking of a typical data
analysis pipeline for a diffraction imaging like the one performed at ESRF ID15
and proposes some disruptive techniques to decode CBF images using the
computational power of graphics cards. 

\end{abstract}


\section{Introduction}

Diffraction imaging is an X-Ray imaging method which uses the cristallinity
as signal where a pencil beam is raster-scanned onto a sample and the
(powder) diffraction signal is recored by a large area detector.
It turns out the current detector are already
fast enough to fill the temporary storage and data analysis \emph{is} the
limiting factor for the whole experiment. 

The analysis of this experiment is often a simple azimuthal regrouping of the
diffraction image into a powder pattern with the intensity given as function of
the scattering vector $q=4 \pi sin (2\theta/2)/\lambda$. 
More complex analysis are possible and often desirable but they are even more
computationally intensive.
%#Before takling advanced analysis, simple one should be evaluated.

We will focus in the second section on the setup of the Materials beam-line
(ID15a) of the ESRF and perform a complete benchmarking of the data analysis 
pipeline used. 
This will highlight various bottle-necks of the data-analysis chain.
To address the image decompression bottlneck, different parallelisation
have been developed and is presented in section 3. 

\section{X-ray diffraction imaging: data analysis pipeline}

A typical diffraction imaging setup is respresented in \ref{}


\subsection{Beamline hardware}

\subsubsection{Pilatus3 2M CdTe detector}

The ID15a beam-line at ESRF uses mainly a Pilatus3 2M detector with a 1000 $\mu
m$ CdTe sensor, manufactured by Dectris \cite{dectris}. 
The detector is made-up of 8x3 Pilatus modules (100 kilopixel each).
Unlike Silicon-based sensors, there are two Cd-Te wafers bump-bound to every
Pilatus-module, with a gap of 3 pixels between the wafers.

This detector is sold with a detector-PC which is in charge of compressing and
saving the the images. 
This detector-PC comes with a 10 Gbit/s network card and is directly connected
to the data analysis server.

The detector is advertised to operate at 250 frames per seconds. 
Each frame being composed of 24 pilatus modules, this makes 2.4 megapixel
images, which are 32-bits integers (the dynamic range is only 20 bits).
If one saves the images as uncompressed raw stream this makes 19.2 gigabits per
second to transfer.
Compression is hence mandatory to transfer the acquired data through the 10
gigabit network interface.
Dectris uses the CIF binary format (cbf) \cite{cbf} for their Pilatus detectors
with byte-offset compression which provides a compression factor close to 4x. 
An alternative compression scheme used in novel Eiger detector is the LZ4 but
the compression factor is much less, around 2x (and variable) which makes
this option un-applicable for operating the Pilatus detector continuously over
an extended period of time.

\subsubsection{Data analysis computer}

The detector computer is directly attached to the network interface of the data
analysis computer. 
This data analysis computer has two processors Intel Xeon E5-2643 v3, each with 
6 cores and  20 Mbyte of cache, and 128 gigabytes of memory. 
There is additionally two 10-gigabit network cards, a fast Intel P3700
solid-state drive, and a Nvidia Titan-X graphics card, all connected on the
PCI-express bus.

\subsection{Processing of diffraction imaging experiment}

The pre-processing for diffraction imaging experiments is typically the
azimuthal integration of the whole image with some mask. 
This snippet of code is a typical example: all images are read, integrated in 1D
profile and saved in a HDF5 file.

\label{sequential}
\begin{verbatim}
import h5py, fabio, pyFAI

def process(list_of_files, result_file, geometry_file, number_of_bins):
    ai = pyFAI.load(geometry)
    with h5py.File(result_file) as result:
        dataset = result.create_dataset((len(list_of_files),
        number_of_bins,dtype='float32') 
        for index, one_file in enumerate(list_of_files):
            with fabio.open(one_file) as fimg:
                data = fimg.data
            res = ai.integrate1d(data, number_of_bins)
           dataset[index] = res.intensity
        result['q'] = res.radial
\end{verbatim}

The function ``process" described in this snippet has been used for
profiling the application (i.e measuring the bottle-neck in performance). 
The input dataset is composed 1202 images in CBF-format coming from an actual
experiment (not synthetic data) written on a (fast) Intel P3700 solid-state
drive (SSD).
One should distinguish two cases: when the data are only on the
solid-state and when they are available in the cache of the
operating system.
The total amount of raw-data is 3 gigabyte, so only the first read can be
considered as a ``cold-start'', subsequent reads are actually benefitting from
the cache of the operating system and should be considered as ``hot-start''.
To be able to profile in cold-start, the disk has been un-mount and re-mounted
to flush all caches, and 5 measurements have been made to select the best one.
The table \ref{Performances} provide the performance in
frames per seconds of various configuration. 
The first and the third line report the performances in cold- and hot-start.
This allows us to evaluate the actual read time per frame: >4 milisecond.

As a consequence, it will be impossible to process images at 250 frames per
second from this solid state drive as it is not possible to read the (raw) data
at the required pace.
There is an emerging technology (3D X-point technology by Intel) for replacing
NAND cells in soldi-state drives which looks promising and should offer
lower latency for acting as a cache for the raw data. 
As of today, those drives are not yet available in capacities large
enough for replacing the SSD for temporary storage.

As online data analysis has to rely on data ``living in memory'' and not
read from the drive, the 128 gigabyte of memory available on the computer
represents a cache of about three minutes of experiment time and the data
analysis pipeline. 
Profiling ``precisely'' the data-analysis program is of crucial importance. 

\subsection{Profiling of the data-analysis pipeline}

The snippet of code \ref{sequential} has been run in a profiler which is
measures how much time is spent in every part of the code. 
The code executes on 1202 images in 43 s and the three most time consuming parts
are: the azimuthal integration (29 s), the byte-offset decompression (4.9 s) and
the checksum calculation (4.7 s).
The checksum verification is optional in CBF and may be simply discarded in the
file reading.

The azimuthal integraton was originally performed on the processor and could be
off-loaded to the graphics card, which lowers the time spent in integration to 
13 s.
As described in \cite{kieffer_ashiotis-proc-euroscipy-2014}, most of the time
for azimuthal integration on graphics card is in the transfer of the raw image
to the device.
To speed up the azimuthal intgration, the best would be to transfer less data to
the graphics card.

Remains the bottle-neck of byte-offset decompression \ldots this algorith will
be described in details in the next section. 

\section{Optimizing the decompression of CBF images}

\subsection{Decompression on the processor}

The core idea of the byte-offset compression is to encode only the difference
value with the previous pixel and hope this value is small enough to fit in a
8-bit (signed) integer. 
Larger values are coded with a special value (-128) which indicates an exception
and subsequent 2 bytes are used to store a 16-bit integer. 
If the value does not fit in a 16-bit integer a 32-bit exception is signaled and
the value is coded on 32 bits. 
With this encoding, one value is coded with variable size on 1, 3 or 7 bytes,
which makes it very difficult to decompress in a parallel fashion.

Since 2004, processor are running at a maximum speed of about 4GHz. 
This means that a serial program is running at the same speed today on a
high-end computer as it does on a dozen-year-old computer.
Since 2004, processors got much more powerful but using parallelism instead
of increasing the clock frequency and we will see now a couple of strategies to
make the processing actually faster.

\subsection{Pool of worker strategy} 

A classical strategy in parallel computing is to attribute one type of
computation to a given compute engine.
Azimythal being already executed on the graphics-card, it is natural to devote
the file reading and decompression to the processor.
A pool of worker (figure \ref{pool}) is set-up using multiple threads for
reading and decoding the data. 
The number of worker in this pool is variable which needs to be optimized 
depending on the computer, especially it depends on the number of processors,
of core, and the amount of cache on the processor.
The list of files to be processed is distributed to the pool of file-reader via
a parallel queue.
Each worker is an infinite loop, waiting for the name of the file to read 
on the input queue and putting the decoded data to the output queue after
reading and decompression.
Later on, azimuthal integration and data saving, using the Nexus-HDF5
format\cite{nexus}, is again performed sequencially.

The table \ref{Performances} provides the number of frames processed per second
when processing a sample of 1202 frames for various size of the pool:
1, 2, 4, 6, 8, 12 and 24 workers, all data being already in memory (hot-start).
This number has to be compared with the frame-rate of the detector of interest:
250 images per second.
The performances of the linear pipeline is given as comparison, in this case the
data can be read from the disk (Intel P3700 SSD, cold-start) or readily
available in the memory, cached by the operating system (linux).

\begin{table}
\caption{Average performances (in frames per second) for the azimuthal
integration of 1202 Pilatus3 2M CdTe images saved in CBF format read from
SSD (or from memory) on a dual Xeon E5-2642v3 with a Nvidia Titan-X graphics
card. Best of 5 runs}
\label{Performances}
\vspace{1mm}
\begin{center}
\begin{tabular}{lcccc}
Strategy & Readers & Integrator & Data source & frames per second \\
\hline 
Sequential & \textsc{cpu} & \textsc{cpu} & \textsc{ssd} & 28.5 \\
Sequential & \textsc{cpu} & \textsc{gpu} & \textsc{ssd} & 54.5 \\
\hline 
Sequential & \textsc{cpu} & \textsc{cpu} & \textsc{mem} & 32.5 \\
Sequential & \textsc{cpu} & \textsc{gpu} & \textsc{mem} & 75.9 \\
\hline 
Pool & 1 & \textsc{gpu} & \textsc{mem} & 171 \\
Pool & 2 & \textsc{gpu} & \textsc{mem} & 202 \\
Pool & 3 & \textsc{gpu} & \textsc{mem} & 180 \\
Pool & 4 & \textsc{gpu} & \textsc{mem} & 206 \\
Pool & 6 & \textsc{gpu} & \textsc{mem} & 188 \\
Pool & 12 & \textsc{gpu} & \textsc{mem} & 134 \\
Pool & 24 & \textsc{gpu} & \textsc{mem} & 90 \\
\hline 
OpenCL & \textsc{gpu} & \textsc{gpu} & \textsc{mem} & 256 \\
\end{tabular}
\end{center}
\end{table}

On can notice the optimal performances are reached with a limited number of
workers in the pool, 2 or 4 workers while the computer has 2x6 cores. 
There are multiple reasons for this:
\begin{itemize}
  \item the main thread is also working: control of the azimuthal integrator
  and saving of the reduced data in HDF5 format.
  \item the amount of cache of each of the two processors is 20 megabytes and
  each frame needs 10 + 2 megabytes to be decoded. More workers means less cache
  for each of them which is detrimental for performances.
  \item Python threads are serialized via the ``global interpreter lock'',
  called \textsc{gil}.
  While most of the processing performed is done in ``no-\textsc{gil}'' section,
  more threads makes more likely the probably to have them fighting each other
  to get access to the \textsc{gil}.
\end{itemize}



I is frustrating to have a powerful parallel processor on the
graphics card and seeing the total performance limited by the file-reading
which is serial.

\subsection{Parallel decompression of CBF images}
 
This section gives a glance of how to decode the byte-offset algorithm, as
implemented in the CBF file-format, using parallel processors and especially
graphics processors.
All threads in a work-group perform the same sequence of operation
on different data (in a SIMD way).
This sequence of operation is called ``kernel'' when executed in parallel, it
looks like a function, written in C-language and operates on a set of data with
a given size called work-group.

Parallel decompression of byte-offset may be divided in those subsequent steps:
\begin{itemize}
  \item search for exception in the input stream (marked as value -128) and
  register them. Other ``normal" values are simply decoded and stored.
  \item treat all blocks of exception sequencially: one thread is treating a
  complete block of adjacent exceptions and multiple treads are processing in
  parallel muliple blocks.
  \item As the delta from the previous value is stored, on needs to calculate
  the cumulative sum of all data. This is performed using a ``scan
  algorithm''\cite{scan}.
  \item The position of each value (in the output array) is also calculated
  using a scan. Technically the two scans are performed simultaneously in our
  implementation.
  \item The decoded, after summation, are copied at the proper place in the
  output array, optionally with a conversion to floating point value.
\end{itemize} 

The implementation of this algorithm is available as part of the silx
\cite{silx} library and will be released in version v0.7 of the library under a
MIT license.
The graphics processor targeted by theis implementation has thousands of cores
(Nvidia Titan-X: 3072 cores) but this algorithm remains valid regardless the
number of cores and the implementation provided in OpenCL \cite{pyopencl} has
been valided on different architectures like integrated graphics processor found
in laptops and on multi-core central processor.

The strength of this approach reside in the limited amount of
data to be sent to the graphics card, which allows decompression and
integration to be performed on the device without additional
transfer over the PCI-express bus (which are always a limiting factor).

The speed-up of this parallel implementation is important (10x),
in comparison with the single-threaded implementation described previously for
large images (Pilatus 6M).
For smaller images, where data are fitting in the processor cache, the serial
algorithm performs great hence the speed-up of the parallel version is
rather limited (+50\%). 
Neverthless, the combination of the parallel cbf-decompression with azimuthal
integration on the graphics card allows to exceed the 250 frames per second
imposed by the detector. 
Moreover, this parallel implementation takes naturally benefits of advances in
graphics card processors (i.e. more core, faster memory): 
the same benchmark has been performed on a desktop computer with only one core
(Xeon 1650v3) instead of 2 , with less cache (15Mb instead of 20Mb) and half
the memory (64Gb instead of 128Gb) but a more recent graphics card (Nvidia GTX
1080Ti instead of Titan-X) and this desktop computer out-performed the server
with more than 300 frames per second.

\section{Outlook}

To be able to inter-connect the decompressed data obtained in the silx library
with the azimuthal integrator provided by pyFAI, without transferring the data
back and forth form the device to the processor, the silx team had to develop a
way to exchange memory objects located on the graphics card between libraries. 
Those results show the validity of the concept and opens the door for
interconnecting different algorithm including image analysis and
tomography algoritms which are available as part of the silx library.
In pyFAI, a couple of advanced statistical analysis, recently ported to
OpenCL like median filtering in 2d-integrated data and sigma-clipped
average could also be good candidates for this kind of direct interconnection.

The other strength of this approach method is that it hides completely the
complexity of byte-offset decompression with a simple object decompression
object (called ``bo'' in the snippet below) which presents the equivelent code
to the one in \ref{sequential}.
This snippet is only a few lines longer than the sequential implementation and
uses the same strategy:

\begin{verbatim}
import h5py, fabio, pyFAI, os
from silx.opencl.codec.byte_offset import ByteOffset

def process(list_of_files, result_file, geometry_file, number_of_bins):
    ai = pyFAI.load(geometry_file)
    data = fabio.open(list_of_files[0]).data
    res = ai.integrate1d(data, number_of_bins, method="ocl_csr_gpu")
    engine = ai.engines["ocl_csr_integr"].engine
    bo = ByteOffset(os.stat(list_of_files[0]).st_size, data.size, ctx=engine.ctx)
    cbf = fabio.cbfimage.CbfImage()
    with h5py.File(result_file) as result:
        dataset = result.create_dataset("intensities", (len(list_of_files), number_of_bins), dtype='float32')
        result['q'] = res.radial
        for index, one_file in enumerate(list_of_files):
            raw = cbf.read(one_file, only_raw=True)
            dec = bo(raw, as_float=True)
            dataset[index] = engine.integrate(dec)[0]
\end{verbatim}


\section{Conclusion}

The processor we are using for analysing data have hit the power wall more then
a  decade ago.
Since then,  no noticeable increase in performances have been seen on
sequential algorithms, causing a bottle-neck in the processing
pipeline for many beamlines, especially for the one doing diffraction imaging.
To be able to cope with whe stread of data coming from modern detector, today's
fastest flash drives are not (yet) fast  enough for acting as an
effective cache and the data should best be kept in memory.
Two types of parallelization have been evaluated to spped-up calculation:  
The ``pool of worker'' strategy has been evaluated for reading and decoding
different images on different processors.
It provides additional preformances compared to the serial implementation but
the speed is not proportional to the number of core of the computer, probably due 
to cache congestion.

The first parallel implementation of the byte-offset decompression 
algorithm is also presented. 
By taking benefit of recent graphics cards, this program allows the reduction
of data for a diffraction imaging experiments performed at the full speed of
a Pilatus3 2M detector, in real time. 
Moreover this implementation is even faster on newer hardware. 

 
\ack{Acknowledgements:}
This work was carried out on the request of Gavin Vaughan and Marco Di
Michiel, scientists on the Materials diffraction beamline at ESRF (ID15a), who
provided the test dataset composed of 1202 images.
In the instrumentation services and development division (ISDD) from ESRF I
would like to acknowledge the help of Sébastien Petitdemange who set-up the
data-transfer from the detector-computer to the data-analysis computer using NFS
over RDMA. 
In the data analyis unit, I would like to thank V.
Armando Solé, head of data analysis unit and leader of the \textit{silx} project, and all our colleagues from the \textit{silx}
project:
Thomas Vincent, Henri Payno, Damien Naudet, Pierre Knobel, Valentin Valls and
Pierre Paleo for their support and ideas.

\bibliographystyle{iucr}
\bibliography{biblio}


\end{document}
