%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2003 International Union of Crystallography
% Version 1.2 (11 December 2002)
%------------------------------------------------------------------------------
%
\documentclass[preprint]{iucr}              % DO NOT DELETE THIS LINE
                   \def\href#1{\relax}\let\foo\caption
\ifPDF
  \RequirePackage{hyperref}
  \PassOptionsToPackage{pdftex,bookmarksopen,bookmarksnumbered}{hyperref}
  \voffset=-0.5in
\fi
\let\caption\foo

\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
 \usepackage{amsmath}
%\usepackage{eps2pdf}

 \paperprodcode{a000000}      % Replace with production code if known
 \paperref{xx9999}            % Replace xx9999 with reference code if known
 \papertype{IU}               % Indicate type of article
 \paperlang{english}          % Can be english, french, german or russian
 \journalcode{S}             % Indicate the journal to which submitted
 \journalyr{2017}
 \journalreceived{\relax}
 \journalaccepted{\relax}
 \journalonline{\relax}

\begin{document}                  % DO NOT DELETE THIS LINE

\title{Real-time diffraction contrast tomography data reduction}
\shorttitle{GPU decompression of CBF images}

 \cauthor[a]{J.}{Kieffer}{jerome.kieffer@esrf.eu}
 \author[a]{S.}{Petitdemange}
 
 \aff[a]{ESRF, The European Synchrotron, CS40220, 38043 \city{Grenoble}
 Cedex 9, \country{France}}
 \shortauthor{Kieffer & Petitdemange}


\keyword{Powder diffraction}
\keyword{Diffraction imaging}
\keyword{Pilatus detector}
\keyword{Image compression}
\keyword{Azimuthal integration}
\keyword{pyFAI}
\keyword{FabIO}
\keyword{silx}



\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Benchmark of the data-analysis pipeline for XRD-CT. CBF byte-offset
decompression on GPU.
\end{synopsis}

\begin{abstract}

Diffraction imaging is an X-Ray imaging method which uses the cristallinity
information (cell parameter, orientation ) as signal to create an images pixel
per pixel:
a pencil beam is raster-scanned onto a sample and the (powder) diffraction
signal is record by a large area detector. 
With the flux provided by of third generation synchrotron and the speed of
hybrid pixel-detector, the acquisition speed of those experiments is now 
limited by the transfer rate to the local storage as the data reduction can
hardly be performed in real time.

This contribution presents the benchmarking of a typical data
analysis pipeline for a diffraction imaging experiment like the one performed at
ESRF ID15a and proposes some disruptive techniques to decode CBF images using the
computational power of graphics cards to be able to perform data reduction in
real-time.
\end{abstract}


\section{Introduction}

Since all major third-generation synchrotrons are undergoing upgrades
to provide brighter sources \cite{ESRF-EBS, S8U} the same
flux of photon will be available soon in much smaller beam. 
Two types of experiments will most benefit from this improved
X-ray source: coherence diffraction experiments and raster-scanning
experiments.

X-Ray diffraction computed tomography (hereafter XRD-CT, \ref{diff_tomo}),
\cite{XRDCT} is one of the raster scanning experiments where a pencil beam is 
scanned onto a sample.
The volumetric information is obtained by rotating the sample in the X-ray beam
to generate the sinogram.
The diffraction signal, scattered over a large solid angle is recorded by a
an area detector and saved as a stack of images.
Those images are azimuthal averaged into powder diffraction patterns with the
intensity given as function of the diffraction angle ($2\theta$) or of the scattering
vector $q=4 \pi sin (2\theta/2)/\lambda$.
The sinogram is built, pixel per pixel, by storing this pattern as a function of
the sample position: translations and rotation.
The tomogram is finally reconstructed from the back-projection of the sinogram.  

\begin{figure}
\label{diff_tomo}
\begin{center}
\includegraphics[width=15cm]{Diffraction_imaging.eps}
\caption{Scheme of a X-Ray diffraction tomography setup where the
sample can be translated and rotated in front of a pencil beam. 
The Debye-Scherrer rings formed by the diffraction of crystallites are collected
by an area-detector, integrated into a powder diffraction pattern and
assembled to create a sinogram, which is computationally back-projected to build
the tomogram.}
\end{center}
\end{figure}

 
It turns out the current detector are already fast enough to fill the temporary
storage and data analysis workflow cannot cope with the pace imposed by
modern detector \cite{GigaFRoST}: data analysis \emph{is} the limiting factor
for the whole experiment.
This work focuses on the performance optimization of the data reduction
pipeline for the cases where the reduction is simply an azimuthal
regrouping of input images.
More complex analysis are possible (and often desirable) but the target of this
work is online data analysis so the analysis will be restricted to simple ones.

We will focus in the second section on the setup of the Materials beam-line
(ID15a) of the ESRF  \cite{id15a} and perform a complete benchmarking of the
data analysis pipeline used. 
This will highlight various bottle-necks in the data-analysis chain.
To address the image decompression bottle-neck, different parallelization
schemes have been developed and are presented in section 3. 

\section{X-ray diffraction imaging: data analysis pipeline}

\subsection{Beamline hardware}

\subsubsection{Pilatus3 2M CdTe detector:}

The ID15a beam-line at ESRF uses mainly a Pilatus3 2M detector with a 1000 $\mu
m$ CdTe sensor, manufactured by Dectris \cite{pilatus}. 
The detector is made-up of 8x3 Pilatus modules (100 kilopixel each).
Unlike Silicon-based sensors, there are two Cd-Te wafers bump-bound to every
Pilatus-module, with a gap of 3 pixels between the wafers.
The gaps between the Piltus modules are the same as the Silicon-based
detectors, i.e. 7 pixels vertically and 17 pixel horizontally.

This detector is sold with a detector-PC which is in charge of compressing and
saving the images on the network. 
This detector-PC comes with a 10 Gbit/s network card and is directly connected
to the data analysis server.

The detector is advertised to operate at 250 frames per seconds. 
Each frame has 2.4 megapixels, stored as 32-bits integers (the dynamic range is
only 20 bits).
At full speed, the raw (uncompressed) stream represents hence 19.2 gigabits per
second to transfer. 
Compression is hence mandatory to transfer the acquired data through the 10
gigabit network interface.
Dectris uses the CIF binary format (cbf) \cite{cbf} for Pilatus detectors
with byte-offset compression which provides a compression factor close to 4x. 
An alternative compression scheme used in the novel Eiger detector is the LZ4
but the compression factor is much less, around 2x (and variable, depending on
the signal of the detector) which makes this option un-applicable for operating
the Pilatus detector continuously over an extended period of time.

\subsubsection{The data analysis computer}
(Figure \ref{computer}) acts as a NFS (network file server) server over RDMA
(remote direct memory access) for optimal performances with the detector and is
directly connected to the GPFS (general parallel file-system)
storage cluster \cite{gpfs}.
This data analysis computer has two processors Intel Xeon E5-2643 v3, each with 
6 cores and  20 MB of cache, and 128 GB of memory. 
There is additionally two 10-gigabit network cards, a fast Intel P3700
solid-state drive (SSD), and a Nvidia Titan-X graphics card, all connected on
the PCI-express bus.

\begin{figure}
\label{computer}
\begin{center}
\includegraphics[width=15cm]{lid15agpu1.eps}
\caption{Data-analysis computer with the main interconnection and their
associated bandwidth}
\end{center}
\end{figure}


\subsection{Processing of diffraction imaging experiment}

The pre-processing for diffraction imaging experiments is typically the
azimuthal integration of the whole image with some mask. 
This snippet of code is a typical
example:
all images are read, integrated in 1D profile and saved in a HDF5 file (here not
following the Nexus \cite{nexus} convention for for sake of concision in this example).

\begin{minipage}{\linewidth}
\label{snippet}
\label{sequential}
\begin{verbatim}
import h5py, fabio, pyFAI

def process(list_of_files, result_file, geometry_file, number_of_bins):
    ai = pyFAI.load(geometry)
    with h5py.File(result_file) as result:
        dataset = result.create_dataset("intensities"
                                        (len(list_of_files), number_of_bins),
                                        dtype='float32') 
        for index, one_file in enumerate(list_of_files):
            with fabio.open(one_file) as fimg:
                res = ai.integrate1d(fimg.data, number_of_bins)
           dataset[index] = res.intensity
        result['q'] = res.radial
\end{verbatim}
\end{minipage}

This piece of code is written in Python \cite{python} programming language and
uses some external libraries:
\begin{itemize}
  \item {H5Py} which is the binding for the HDF5 library in Python \cite{h5py}
  \item{FabIO} which is a library for reading most of the X-ray images format
  \cite{fabio}
  \item{PyFAI} which is the library which performs the azimuthal
  integration from 2D images to 1D profile \cite{pyFAI}. 
\end{itemize}

The function ``process" described in this snippet has been used for
profiling the application (i.e measuring the bottle-neck in performance). 
The input dataset is composed 1202 images in CBF-format coming from an actual
experiment on ID15a written on the SSD.
One should distinguish two cases: when the data are only on the
solid-state and when they are available in the cache of the
operating system.
The total amount of raw-data is 3 gigabyte, so only the first read can be
considered as a ``cold-start'', subsequent reads are actually benefitting from
the cache of the operating system and should be considered as ``hot-start''.
To be able to profile in cold-start, the disk has been un-mount and re-mounted
to flush all caches.
Each measurement has been performed five times and the results are reported in
the table \ref{Performances}.  Only the median framerate  with the median
deviation to this median have been reported in frames per second (or Hertz).

The first and the third (resp. second and fourth) line report the performances
in cold- and hot-start.
This allows us to evaluate the actual read time per frame from the flash
drive which is 4 millisecond (resp. 5 ms).

As a consequence, it is impossible to process images at 250 frames per
second from this SSD as it is not possible to read the (compressed) data
at the required pace.
There is an emerging technology (3D X-Point technology by Intel) for replacing
NAND cells in SSDs which looks promising and should offer
lower latency for acting as a cache for the raw data. 
As of today, those drives are not yet available in capacities large
enough for replacing the memory for the kind of temporary storage needed for
beam-line application.

As online data analysis has to rely on data ``living in memory'' and not
read from any drive, the 128 gigabyte of memory available on the computer
represents a cache of about three minutes of experiment time and the data
analysis pipeline has to keep the pace of the experiment. 
Profiling ``precisely'' the data-analysis program is hence of crucial
importance.

\subsection{Profiling of the data-analysis pipeline}

The snippet of code from section \ref{snippet} has been run in a profiler which
measures how much time is spent in every part of the code, over a reference
dataset which is composed of 1202 compressed images.
The code executes on this reference dataset in 43 s and the
three most time consuming parts are: the azimuthal integration (29 s), the
byte-offset decompression (4.9 s) and the checksum calculation (4.7 s).
The checksum verification is optional in CBF and may simply be discarded
during the file reading.

The azimuthal integration was originally performed on the processor and could be
off-loaded to the graphics card, which lowers the time spent in integration to 
13 s.
As described in \cite{kieffer_ashiotis-proc-euroscipy-2014}, most of the time
for azimuthal integration on graphics card is in the transfer of the raw image
to the device.
To speed up the azimuthal integration, the best would be to transfer less data
to the graphics card, i.e the compressed data and decompress them on the GPU.

Remains the bottle-neck of byte-offset decompression \ldots this algorithm will
be described in details in the next section and analyzed. 

\section{Optimizing the decompression of CBF images}

\subsection{Decompression on the processor}

The core idea of the byte-offset compression is to encode only the difference
value between two adjacent pixels and hope this value is small enough to fit in
a 8-bit (signed) integer, i.e in the range -127 to +127.
Larger values are coded with a special value (-128) which indicates an exception
and the subsequent 2 bytes are decoded as a 16-bit integer in little endian
order.
If the value does not fit-in a 16-bit integer, a 32-bit exception is
signaled (with value -32768) and the actual value is coded over the 4 next bytes
as a 32 bits little-endian integer.
Hence, each value can be coded with a variable size on 1, 3 (=1+2) or 7 (=1+2+4)
bytes, which makes it very difficult to decompress in a parallel fashion.

Since 2004, processor are running at a maximum speed of about 4GHz. 
This means that a serial algorithm like the byte-offset decompression described
previously is running at the same speed today on a high-end computer as it
does on a 13-year-old computer.
Parallelization is the only way to get the processing done faster and a couple
of strategies have been explored and will be presented.

\subsection{Pool of workers strategy} 

A classical strategy in parallel computing is to attribute one type of
computation to a given compute engine.
Azimuthal averaging being already executed on the graphics-card, it is natural
to devote the image decompression to the central processor (CPU).

On lower-end (disk-) drives, the data access is serialized to a single
I/O queue and the best is to have read operations performed in the main
thread, then subsequently decompression performed in mutiple threads.
The SSD used in this experiment is interfaced in PCI-express using the NVME
protocol \cite{nvme} which provides thousands of parallel queues for reading
and writing.
We validated that the performances are actually better when the reading is
performed within the multiple threads.

A pool of workers (figure \ref{pool}) is set-up using multiple threads for
reading and decoding the data. 
The number of worker in this pool is the parameter which needs to be
optimized depending on the computer, especially as function of the
number of processors, of cores, and the amount of cache available.
The list of files to be processed is distributed to the pool of reader via
a parallel queue.
Each worker is an infinite loop running in its own thread, waiting for the name
of the file to read on the input queue and putting the decompressed data to
the output queue.
Later on, azimuthal integration and data saving, using HDF5 is again performed
sequentially, but the order of the files is not necessary  the same in the
output queue as it was in the input queue. 

\begin{figure}
\label{pool}
\begin{center}
\includegraphics[width=15cm]{pool_of_worker.eps}
\caption{Workflow for the data reduction of a XRD-CT experiment using the pool
of workers pattern.}
\end{center}
\end{figure}


The table \ref{Performances} provides the number of frames processed per second
when processing the sample set of frames and varying the size of the
pool of readers:
1, 2, 3, 4, 6, 12 and 24 workers, all data being already in memory
(hot-start).
This number has to be compared with the frame-rate of the detector of interest:
250 fps.
The performances of the linear pipeline is given as comparison, in this case the
data can be read from the disk (Intel P3700 SSD, cold-start) or readily
available in the memory, cached by the operating system (linux).

\begin{table}
\caption{Average performances (in fps) for the azimuthal
integration of 1202 Pilatus3 2M CdTe images saved in CBF format read from
SSD (or from memory) on a dual Xeon E5-2642v3 with a Nvidia Titan-X graphics
card. Median over 5 runs and median deviation to the median.}
\label{Performances}
\vspace{1mm}
\begin{center}
\begin{tabular}{lccccc}
Strategy & Readers & Integrator & Data source & median fps  & deviation\\
\hline 
Sequential & \textsc{cpu} & \textsc{cpu} & \textsc{ssd} & 28.2 &0.3\\
Sequential & \textsc{cpu} & \textsc{gpu} & \textsc{ssd} & 53.1 &1.4\\
\hline 
Sequential & \textsc{cpu} & \textsc{cpu} & \textsc{mem} & 31.6 &0.9\\
Sequential & \textsc{cpu} & \textsc{gpu} & \textsc{mem} & 74.6 &1.3\\
\hline 
Pool & 1 & \textsc{gpu} & \textsc{mem} & 167 &3\\
Pool & 2 & \textsc{gpu} & \textsc{mem} & 192 &7\\
Pool & 3 & \textsc{gpu} & \textsc{mem} & 155 &5\\
Pool & 4 & \textsc{gpu} & \textsc{mem} & 190 &4\\
Pool & 6 & \textsc{gpu} & \textsc{mem} & 168 &18\\
Pool & 12 & \textsc{gpu} & \textsc{mem} & 121 &4\\
Pool & 24 & \textsc{gpu} & \textsc{mem} & 88 &2\\
\hline 
OpenCL & \textsc{gpu} & \textsc{gpu} & \textsc{mem} & 253 &2\\
\end{tabular}
\end{center}
\end{table}

It is noticeable the optimal performance is reached with a number of
workers in the pool much lower than the number of cores of the computer, 2 or 4
readers is optimal while the computer has 2x6 cores.
There are multiple reasons for this:
\begin{itemize}
  \item the main thread is also working: it controls the azimuthal
  integrator and the saving of the reduced data in HDF5 format.
  \item the amount of cache of each of the processor, which is 20 megabytes, has
  to be compared with the 2 + 10 megabytes for each frame (encoded + decoded).
  More readers means less CPU-cache for each of them which is detrimental
  for performances.
  \item Python threads are serialized via the ``global interpreter lock'',
  called \textsc{gil}.
  While most of the processing performed is done in ``no-\textsc{gil}'' section,
  more threads makes more likely to have them fighting each other
  for getting access to the \textsc{gil}.
\end{itemize}

It is frustrating to have a powerful parallel processor on the
graphics card and seeing the total performance limited by the file-reading
which is purely serial.

\subsection{Parallel decompression of CBF images}
 
This section gives an overview of an implementation of the byte-offset
decompression on massively parallel processors like graphics processors
(GPU) using the OpenCL \cite{opencl} programming language.
In those devices, threads are regrouped in ``work-groups'' of given size. 
All threads from a work-group perform the same sequence of operation on
different data (in a SIMD way) which is called a ``kernel'', it looks like a
function, written in C-language.

Parallel decompression of byte-offset may be divided in those subsequent steps,
each of them being divided into one or two kernels to be called subsequently:
\begin{itemize}
  \item search for exception in the input stream (marked with value -128) and
  register them. Other ``normal" values are simply decoded and stored.
  \item treat all sections of exceptions in parallel: Here the workgroup size is
  one, so one thread is treating a complete section of adjacent exceptions
  and multiple treads are processing in parallel multiple sections. If a thread
  is starting in the middle of an exception sections, it does nothing as this
  region has to be treated by the thread which starts at the begining of the 
  exception section.
  \item As the difference from the previous pixel value is stored, one needs to
  calculate the cumulative sum of all data. This is performed using a
  ``prefix-sum algorithm'' \cite{scan}.
  \item The position of each value (in the output array) is also calculated
  using the prefix-sum algorithm: Valid pixel position are set to one in the
  input and other remains at zero. This algorithm provides the output pixel
  position for any input position.
  Technically the two prefix-sums are performed simultaneously in our
  implementation.
  \item The decoded values, after summation, are copied at the proper place in
  the output array, optionally with a conversion to floating point value to
  ease subsequent processing.
\end{itemize} 

The implementation of this algorithm is available as part of the silx
\cite{silx} library and will be part of the version 0.7.
While GPUs targeted by this implementation have thousands of
cores, this algorithm remains valid regardless the
number of cores.
Our implementation, based on pyOpenCL \cite{pyopencl}, 
has been validated on different architectures like Nvidia GPUs, on 
integrated graphics processor (Intel Iris) found in laptops and on multi-core
processor (Intel, AMD and Apple OpenCL drivers). 

The strength of this approach resides in the limited amount of
data to be sent to the GPU memory, which allows decompression and
integration to be performed on the device without additional
transfer over the PCI-express bus (which are the bottleneck for pyFAI).

The performance of is parallel decompression of CBF images on high-end GPU has
been compared with the serial implementation. 
For large images which do not fit into the cache memory of the
processor (typically for the Pilatus 6M images), the speed-up of this GPU
version is important (10x).
For smaller images, where data are fitting in the processor cache, the serial
algorithm performs great hence the speed-up of the parallel version is
rather limited (+50\%). 
Nevertheless, the combination of the parallel CBF-decompression with azimuthal
integration on the graphics card allows to exceed the 250 frames per second
imposed by the detector as reported on the last line of the table
\ref{Performances}.
Moreover, this parallel implementation takes naturally benefits of advances in
graphics card processors (i.e. more core and faster memory): 
the same benchmark has been performed on a desktop computer with only one
processor (Xeon 1650v3) instead of 2, with less cache (15MB instead of 20MB),
half the memory (64 GB instead of 128 GB) and a cheaper graphics card while more
recent(Nvidia GTX 1080Ti instead of Titan-X) and this desktop computer
out-performed the server with more than 300 frames per second in this benchmark.
This parallel algorithm is not only faster than the serial version, it is also
much more stable in performances as the variability (expressed
as the median of absolute difference to the median value) is only of 2 fps on 
the GPU and twice more for the pool of workers pattern. 
The stability of the performances on the GPU can be explained by the dedication
of the graphics card to this calculation.

\section{Outlook}

To be able to inter-connect the decompressed data obtained in the silx library
with the azimuthal integrator provided by pyFAI, without transferring the data
back and forth from the device to the processor, the silx team had to develop a
way to exchange memory objects located on the graphics card between libraries. 
Those results show the validity of the concept and opens the door for
interconnecting different algorithm including image analysis and
tomography algoritms which are available as part of the silx library.
In pyFAI, a couple of advanced statistical analysis, recently ported to
OpenCL like median filtering in 2d-integrated data and sigma-clipped
average could also be good candidates for this kind of direct interconnection.

The other strength of this approach is that it hides completely the
complexity of byte-offset decompression with a simple ``decompression
object'' (called ``bo'' in the snippet below) which presents the equivalent
code to the one in \ref{sequential}.
This snippet is only a few lines longer than the sequential implementation and
uses the same strategy:

\begin{minipage}{\linewidth}
\begin{verbatim}
import h5py, fabio, pyFAI, os
from silx.opencl.codec.byte_offset import ByteOffset

def process(list_of_files, result_file, geometry_file, number_of_bins):
    ai = pyFAI.load(geometry_file)
    data = fabio.open(list_of_files[0]).data
    res = ai.integrate1d(data, number_of_bins, method="ocl_csr_gpu")
    engine = ai.engines["ocl_csr_integr"].engine
    bo = ByteOffset(os.path.getsize(list_of_files[0]), data.size,
                    ctx=engine.ctx) 
    cbf = fabio.cbfimage.CbfImage()
    with h5py.File(result_file) as result:
        dataset = result.create_dataset("intensities", 
                                       (len(list_of_files), number_of_bins),
                                        dtype='float32') 
        result['q'] = res.radial 
        for index, one_file in enumerate(list_of_files):
            raw = cbf.read(one_file, only_raw=True)
            dec = bo(raw, as_float=True)
            dataset[index] = engine.integrate(dec)[0]
\end{verbatim}
\end{minipage}

\section{Conclusion}

Processors used for analysing data have hit the power wall more then
a  decade ago.
Since then,  no noticeable increase in performances have been seen on
sequential algorithms, causing a bottle-neck in the processing
pipeline for many beam-lines, especially for the one doing diffraction imaging.
To be able to cope with the stream of data coming from modern detector,
today's fastest SSD drives are not (yet) fast  enough for acting as an
effective cache and the data should best be kept in memory.
Two types of parallelization have been evaluated to speed-up the processing:  
The ``pool of workers'' strategy has been evaluated for reading and decoding
different images on different cores, in parallel.
It provides additional performances compared to the serial implementation but
the speed is not, by far, proportional to the number of cores of the computer,
probably due to cache congestion.

The first parallel implementation of the byte-offset decompression 
algorithm is also presented. 
By taking benefit of recent graphics cards, this code allows the reduction
of data for a diffraction imaging experiments performed at the full speed of
a Pilatus3 2M detector, in real time. 
Moreover this implementation is even faster on newer hardware. 

 
\ack{Acknowledgements:}
This work was carried out on the request of Gavin Vaughan and Marco Di
Michiel, scientists on the Materials diffraction beam-line at ESRF (ID15a), who
provided the test dataset composed of 1202 images taken at their beam-line with
the Pilatus 2M.
In the software group of the instrumentation services and development division
(ISDD) from ESRF, I would like to acknowledge the help of Sébastien Petitdemange
who set-up the data-transfer from the detector-computer to the data-analysis
computer using NFS over RDMA. 
I would like to thank V. Armando Solé, head of data analysis unit and leader of
the \textit{silx} project. 
The parallel byte-offset decompression algorithm has been reviewed by Thomas
Vincent, architect of \textit{silx}. Without his help, the code would not yet
run under Apple computer.
Finally, I would like to acknowledge the contribution of all the members of
the \textit{silx} project, past and present:
Henri Payno, Damien Naudet, Pierre Knobel, Valentin Valls and
Pierre Paleo for the great team work.

\bibliographystyle{iucr}
\bibliography{biblio}

\end{document}
